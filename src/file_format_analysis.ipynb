{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File format analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to approach the project with a broader scope, we have not only decided to focus on the data analysis component but also some aspects regarding the data management of a possible data exploitation process this project could be encapsulated into.\n",
    "\n",
    "We have defined different data zones to store the different steps of the data analysis process. Although we want the solution to be very general to fit most cases, we will also try to propose suitable approach to storing this data.\n",
    "\n",
    "We have seen that lots of the values to be stored are repeated (i.e. the measurement unit of energy which is the same each row), so it makes sense to use some kind of file format that uses compression. For this reason we have decided to test parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "filepath = '../data/test_file_format/gen_DE_B18'\n",
    "\n",
    "# Create an empty DataFrame with columns\n",
    "columns = ['Reading time (s)', 'Storage space (KB)']\n",
    "df_comparison = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Reading a trivial original data file\n",
    "df = pd.read_csv(f'{filepath}.csv', sep=',', decimal='.', encoding='utf-8')\n",
    "df.head(5)\n",
    "\n",
    "# Setting the correct types\n",
    "df.StartTime = pd.to_datetime(df.StartTime, format='%Y-%m-%dT%H:%M%zZ')\n",
    "df.EndTime = pd.to_datetime(df.EndTime, format='%Y-%m-%dT%H:%M%zZ')\n",
    "df.AreaID = df.AreaID.astype(str)\n",
    "df.UnitName = df.UnitName.astype(str)\n",
    "df.PsrType = df.PsrType.astype(str)\n",
    "df.quantity = df.quantity.astype(int)\n",
    "\n",
    "# Creating some test files\n",
    "compression_algorithms = ['none', 'snappy', 'gzip', 'brotli']\n",
    "df.to_csv(f'{filepath}_csv.csv', index = False, encoding = 'utf-8', sep = ',')\n",
    "for ca in compression_algorithms:\n",
    "    df.to_parquet(f'{filepath}_parquet_{ca}.parquet', index = False, engine = 'pyarrow', compression = ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are duplicating data, for completion purposes we will store the CSV again and check some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the test files and storing the time\n",
    "start_time = time.time()\n",
    "df_csv = pd.read_csv(f'{filepath}.csv', sep=',', decimal='.', encoding='utf-8')\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate reading time\n",
    "reading_time = end_time - start_time\n",
    "    \n",
    "# Get file size in KB\n",
    "file_size_bytes = os.path.getsize(f'{filepath}.csv')\n",
    "file_size_kb = file_size_bytes / 1024  # Convert bytes to KB\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Appending data to the comparison dictionary\n",
    "comparison_data.append({\n",
    "    'File format': 'CSV',\n",
    "    'Reading time (s)': reading_time,\n",
    "    'Storage space (KB)': file_size_kb\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also check some metrics for the parquet format using different compression algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterating over the different parquet options\n",
    "for ca in compression_algorithms:\n",
    "    file = f'{filepath}_parquet_{ca}.parquet'\n",
    "        \n",
    "    start_time = time.time()\n",
    "    df = pd.read_parquet(file, engine = 'pyarrow')\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate reading time\n",
    "    reading_time = end_time - start_time\n",
    "    \n",
    "    # Get file size in KB\n",
    "    file_size_bytes = os.path.getsize(file)\n",
    "    file_size_kb = file_size_bytes / 1024  # Convert bytes to KB\n",
    "    \n",
    "    # Append data to the comparison dictionary\n",
    "    comparison_data.append({\n",
    "        'File format': f'Parquet - {ca}',\n",
    "        'Reading time (s)': reading_time,\n",
    "        'Storage space (KB)': file_size_kb\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "df_comparison['Ratio (seconds)/(KB)'] = df_comparison['Reading time (s)']/df_comparison['Storage space (KB)']\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this small example, we can see how CSV is greatly inferior in terms of both reading speed and storage space. In addition, parquet stores the types of the different columns, so there is no apparent downside to using it for this project. In particular we have decided to use snappy as it has a better ratio compared to the other checked options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
